# ğŸ§  Calculator-ML

This project demonstrates how a **neural network** can simulate a basic calculator. It serves as an **educational tool** to explore how **Artificial Neural Networks (ANNs)** learn to perform arithmetic operations like addition and subtraction through structured data and training.

---

## ğŸ¯ Objective

Train a neural network to predict the result of a basic arithmetic expression composed of:

* **Numerator (`a`)**: Integer between 1 and 1000
* **Operator (`op`)**: Either addition (+) or subtraction (-)
* **Denominator (`b`)**: Integer between 1 and 1000

Given a, op, and b, the network learns to predict a + b or a - b.

---

## ğŸ“Š Dataset Generation

The dataset is generated using `data_generation.py`. It produces **10,000 examples**, each consisting of:

* Random integers `a` and `b` between 1 and 1000
* A randomly chosen operator (`+` or `-`)
* A **computed result** using Python's `eval()` function

Each line in the CSV file looks like this:

```
numerator,operator,denominator,result
42,+,58,100
75,-,50,25
```

---


## ğŸ§± Model Architecture

Operators are encoded using **one-hot encoding** to treat them as categorical variables:

```
+ â†’ [1, 0]
- â†’ [0, 1]
```

This approach ensures:

* Operators are treated as **categorical**, not ordinal
* The model learns distinct rules for each operator independently
* No bias due to positional proximity (e.g. `+` vs `-`)

### Model Type
### Network Structure

A simple **fully connected feedforward network** (ANN) is used, with:

* **Input size**: 4 (numerator + operator one-hot + denominator)
* **Hidden layers**: 3 layers with 32 â†’ 16 â†’ 16 neurons
* **Activations**: ReLU activation for non-linearity
* **Output**: A single neuron predicting the result (regression)
* **Loss**: Mean Squared Error (MSELoss)
* **Optimizer**: Adam

Although, this project uses an **Artificial Neural Network (ANN)** since the input is a **fixed-length vector** (not sequential data)

---

## ğŸ§  Technical Justification

| Component               | Choice         | Justification                                                            |
| ----------------------- | -------------- | ------------------------------------------------------------------------ |
| **Input Size**          | 4 features     | Numerator (1) + Operator one-hot (2) + Denominator (1)                   |
| **Hidden Layers**       | 32+ 2 Ã— 16 neurons | Sufficient to model  without overfitting |
| **Activation**          | ReLU           | Fast, efficient, avoids vanishing gradients                              |
| **Output Layer**        | 1 neuron       | Predicts a single continuous result                                      |
| **Loss Function**       | MSELoss        | Ideal for regression; penalizes large errors more heavily                |
| **Optimizer**           | Adam           | Adaptive learning rate, no manual tuning required                        |
| **Steps / Batch Size** | 15000 / 64       | Balanced training time and stability for small datasets                  |

---

## ğŸ‹ï¸ Training

The training loop is implemented using **PyTorch** with step-by-step logging and evaluation:

* **Batch size**: 64
* **Training steps**: 15000
* **Evaluation every**: 100 steps
* **Tolerance threshold**: 5% (for accuracy metric)

Metrics tracked include:

* Training loss (MSE)
* Validation loss
* Mean Absolute Error (MAE)
* Tolerance-based accuracy

---

## ğŸ“ˆ Results

After training, the model is evaluated using several visual plots

<div style="display: flex; flex-wrap: wrap; justify-content: space-between; gap: 10px;">
  <img src="output/loss_curve.png" alt="Loss Curve" width="48%">
  <img src="output/accuracy_per_operator.png" alt="Accuracy per Operator" width="48%">
  <img src="output/error_distribution.png" alt="Error Distribution" width="48%">
  <img src="output/residual_plot.png" alt="Predicted vs True" width="48%">
</div>


---

## ğŸš€ How to Run


```bash
# 1. Generate a balanced dataset with + and -
python data_generation.py

# 2. Train the model using PyTorch
python train.py
```

Output files (plots and model checkpoint) will be saved in the `output/` and `models/` directories.

Claro, aquÃ­ tienes una versiÃ³n mejor redactada y mÃ¡s clara de esa secciÃ³n:

---

## ğŸ“ Notes

This project is designed purely for **educational purposes**, with the goal of demonstrating how neural networks can learn simple mathematical patterns through data.

### ğŸ”® Potential Future Improvements

* Extend support to additional operators such as `*`, `/`, and `**` using ditribution normalization on dataset 
* Enable learning with **multi-digit** and **larger numerical ranges**
* Add support for **floating-point numbers** to improve real-world applicability
* Implement **expression parsing** for multi-step operations (e.g., `a + b * c`)
* Compare performance with **symbolic approaches** (e.g., using eval vs learned)

---

## Folder Structure

```
.
â”œâ”€â”€ data_generation.py
â”œâ”€â”€ train.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataset.csv
â”œâ”€â”€ model/
â”‚   â””â”€â”€ calculator_model.pth
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ accuracy_per_operator.png
â”‚   â”œâ”€â”€ error_distribution.png
â”‚   â”œâ”€â”€ loss_curve.png
â”‚   â””â”€â”€ residual_plot.png
```

---

## ğŸ‘¨â€ğŸ’» Author

Created by **Francisco JosÃ© MartÃ­n**
Feel free to reach out for ideas, feedback, or collaboration!
